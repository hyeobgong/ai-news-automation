import os
import json
import re
import time
from google import genai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Gemini ì„¤ì • (google-genai v1.0 ì´ìƒ)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY not found in .env file")

# ìµœì‹  google-genai íŒ¨í‚¤ì§€ ì‚¬ìš© ë°©ì‹
client = genai.Client(api_key=GEMINI_API_KEY)

def deduplicate_news(news_list, similarity_threshold=0.6):
    """
    ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ/ìœ ì‚¬ ê¸°ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    ì„ê³„ê°’ì„ 0.6ìœ¼ë¡œ ë‚®ì¶”ì–´ ë” ë§ì€ ì¤‘ë³µì„ ì¡ë„ë¡ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.
    """
    if not news_list:
        return []
    
    # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œëª©ë§Œ ì‚¬ìš©)
    texts = [item['title'] for item in news_list]
    
    # 2. TF-IDF ë²¡í„°í™”
    # í•œê¸€ ì²˜ë¦¬ëŠ” ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨í•˜ê²Œ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë²¡í„°í™”
    # analyzer='char_wb', ngram_range=(2,3) -> ì² ìê°€ ë¹„ìŠ·í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ì¸ì‹
    vectorizer = TfidfVectorizer(min_df=1, analyzer='char_wb', ngram_range=(2,3))
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    # 4. ì¤‘ë³µ ì œê±° ì¤‘...
    unique_news = []
    visited = [False] * len(news_list)
    
    # ë””ë²„ê¹…: ìœ ì‚¬ë„ í–‰ë ¬ ì¶œë ¥
    # print("Similarity Matrix:\n", similarity_matrix)
    
    for i in range(len(news_list)):
        if visited[i]:
            continue
        
        # í˜„ì¬ ê¸°ì‚¬ë¥¼ ëŒ€í‘œ ê¸°ì‚¬ë¡œ ì„ ì •
        representative = news_list[i]
        related_articles = []
        
        visited[i] = True
        
        for j in range(i + 1, len(news_list)):
            if not visited[j]:
                sim = similarity_matrix[i][j]
                if sim >= similarity_threshold:
                    visited[j] = True
                    related_articles.append(news_list[j])
                    print(f"  [ì¤‘ë³µì œê±°] '{news_list[i]['title']}' ìœ ì‚¬ ê¸°ì‚¬ ì œê±°: '{news_list[j]['title']}' ({sim:.2f})")
        
        # ë¬¶ì¸ ê¸°ì‚¬ ì •ë³´ ì €ì¥
        representative['related_links'] = [r['link'] for r in related_articles]
        unique_news.append(representative)
        
    print(f"ğŸ“‰ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(news_list)} -> {len(unique_news)} ê±´ (ìœ ì‚¬ë„ ê¸°ì¤€: {similarity_threshold})")
    return unique_news

def analyze_news_with_gemini(news_item):
    """
    Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
    """
    prompt = f"""
    ë‹¹ì‹ ì€ AI ë‰´ìŠ¤ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
    
    [ë‰´ìŠ¤ ì •ë³´]
    ì œëª©: {news_item['title']}
    ë§í¬: {news_item['link']}
    ë°œí–‰ì¼: {news_item['published']}
    
    [ìš”ì²­ ì‚¬í•­]
    1. **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**: ë‹¤ìŒ ë‘ ê·¸ë£¹ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³ , ì„¸ë¶€ ì£¼ì œë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
       - ê·¸ë£¹1 (ê¸°ìˆ  ì¤‘ì‹¬): LLM ëª¨ë¸, ì„œë¹„ìŠ¤/ì—ì´ì „íŠ¸, í•˜ë“œì›¨ì–´/í”¼ì§€ì»¬AI ë“±
       - ê·¸ë£¹2 (ì‚¬íšŒ/ê²½ì œ ì¤‘ì‹¬): ë¹„ì¦ˆë‹ˆìŠ¤/íˆ¬ì, ê·œì œ/ìœ¤ë¦¬ ë“±
       
    2. **ì œëª© ë²ˆì—­**: ê¸°ì‚¬ì˜ ì œëª©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

    3. **3ì¤„ ìš”ì•½**: ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
    (ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì½”ë“œë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSON.)
    
    [ì¶œë ¥ ì˜ˆì‹œ]
    {{
        "title_ko": "ë²ˆì—­ëœ í•œêµ­ì–´ ê¸°ì‚¬ ì œëª©",
        "category_group": "ê·¸ë£¹1 (ê¸°ìˆ  ì¤‘ì‹¬)",
        "topic": "LLM ëª¨ë¸",
        "summary": ["ì²«ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤.", "ë‘ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤.", "ì„¸ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤."],
        "importance": 5
    }}
    """
    
    # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (ìµœëŒ€ 3íšŒ, ì§€ìˆ˜ ë°±ì˜¤í”„)
    max_retries = 3
    retry_delay = 10
    
    for attempt in range(max_retries):
        try:
            print(f"  - Gemini ìš”ì²­ ì¤‘... ({news_item['title'][:15]}...) [ì‹œë„ {attempt+1}/{max_retries}]")
            
            # ëª¨ë¸: 'gemini-2.0-flash-lite' ì‚¬ìš©
            response = client.models.generate_content(
                model="gemini-2.0-flash-lite", 
                contents=prompt,
            )
            
            text = response.text
            # ì½”ë“œ ë¸”ë¡(```json) ì œê±°
            text = re.sub(r"```json|```", "", text).strip()
            analysis = json.loads(text)
            
            # ì„±ê³µ ì‹œ 1ì´ˆ ëŒ€ê¸° í›„ ë°˜í™˜ (ì†ë„ë¥¼ ë¹ ë¥´ê²Œ ì¡°ì •)
            time.sleep(1) 
            return analysis
            
        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                print(f"    âš ï¸ í• ë‹¹ëŸ‰ ì´ˆê³¼(429). {retry_delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(retry_delay)
                retry_delay *= 2  # ëŒ€ê¸° ì‹œê°„ 2ë°°ë¡œ ì¦ê°€ (Exponential Backoff)
            else:
                print(f"    âš ï¸ ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break  # 429ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ì¤‘ë‹¨
    
    # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
    print("  âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    return {
        "category_group": "ë¯¸ë¶„ë¥˜ (ë¶„ì„ ì‹¤íŒ¨)",
        "topic": "API ì˜¤ë¥˜",
        "summary": ["AI ë¶„ì„ì— ì‹¤íŒ¨í•˜ì—¬ ì›ë¬¸ ë§í¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.", "API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì…ë‹ˆë‹¤.", "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."],
        "importance": 1
    }

def process_news(news_list):
    """
    ì „ì²´ ë‰´ìŠ¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    """
    # 1. ì¤‘ë³µ ì œê±°
    unique_news = deduplicate_news(news_list)
    
    processed_news = []
    
    print("\nğŸ§  Gemini AI ë¶„ì„ ì‹œì‘...")
    
    # ìƒìœ„ 20ê°œ ë‰´ìŠ¤ ë¶„ì„ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
    processed_count = 0
    max_process = 20
    
    total_to_process = min(len(unique_news), max_process)
    print(f"ğŸ‘‰ ì´ {total_to_process}ê°œì˜ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n")

    for i, news in enumerate(unique_news): 
        if processed_count >= max_process:
            break
            
        print(f"[{processed_count + 1}/{total_to_process}] ë¶„ì„ ì¤‘: {news['title'][:30]}...")
        analysis = analyze_news_with_gemini(news)
        
        if analysis:
            # ì›ë³¸ ë‰´ìŠ¤ ê°ì²´ì— ë¶„ì„ ê²°ê³¼ ë³‘í•©
            news.update(analysis)
            
            # í•œêµ­ì–´ ì œëª©ì´ ìˆìœ¼ë©´ ì ìš©
            if 'title_ko' in analysis:
                 news['title'] = analysis['title_ko']

            processed_news.append(news)
            print(f"  âœ… ì™„ë£Œ! -> {news['title']}")
            processed_count += 1
            
    return processed_news

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ê°€ìƒ)
    test_data = [
        {'title': 'OpenAI, ìƒˆë¡œìš´ GPT-5 ëª¨ë¸ ì¶œì‹œ ì„ë°•', 'link': 'http://example.com/1', 'published': '2026-02-18'},
        {'title': 'OpenAI GPT-5 ì¶œì‹œ ì˜ˆì •, ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ', 'link': 'http://example.com/2', 'published': '2026-02-18'}, # ì¤‘ë³µ
        {'title': 'êµ¬ê¸€ ì œë¯¸ë‚˜ì´, í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ ì§„ì¶œ', 'link': 'http://example.com/3', 'published': '2026-02-17'},
        {'title': 'AI ê·œì œ ë²•ì•ˆ êµ­íšŒ í†µê³¼, ê¸°ì—…ë“¤ ë¹„ìƒ', 'link': 'http://example.com/4', 'published': '2026-02-16'},
    ]
    
    print("--- í…ŒìŠ¤íŠ¸ ì‹œì‘ ---")
    results = process_news(test_data)
    
    print("\n--- ìµœì¢… ê²°ê³¼ (JSON) ---")
    print(json.dumps(results, indent=2, ensure_ascii=False))
